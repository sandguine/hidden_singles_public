{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_path = '/home/ajhnam/projects/hidden_singles_public/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(proj_path + 'python/')\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.distributions.uniform import Uniform\n",
    "from torch.utils.data import DataLoader as DataLoader\n",
    "\n",
    "from hiddensingles.misc import torch_utils as tu\n",
    "from hiddensingles.misc import utils, TensorDict, TensorDictDataset, nnModule, MLP\n",
    "from hiddensingles.experiment.sudoku_hs_service import create_tutorial, create_phase1, create_phase2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_responses(puzzle_data_path, subject_data_path, solvers=True, device='cpu'):\n",
    "    \"\"\"\n",
    "    Returns a tensor of shape [num_solvers, num_trials]\n",
    "    containing the response types of each subject for each trial\n",
    "    \"\"\"\n",
    "    puzzle_df = pd.read_csv(proj_path + puzzle_data_path, sep='\\t')\n",
    "    subject_df = pd.read_csv(proj_path + subject_data_path, sep='\\t')\n",
    "    subject_df = subject_df[~subject_df.solver.isna()]\n",
    "    subject_df.solver = subject_df.solver.astype(bool)\n",
    "    subject_df = subject_df[subject_df.solver if solvers else ~subject_df.solver]\n",
    "    subject_ids = set(subject_df.subject_id)\n",
    "\n",
    "    response_map = {'inhouse': 0,\n",
    "                    'absent': 1,\n",
    "                    'distractor': 2,\n",
    "                    'target': 3}\n",
    "    puzzle_df = puzzle_df[(puzzle_df.phase == 1) & (puzzle_df.subject_id.isin(subject_ids))]\n",
    "    num_trials = len(puzzle_df.trial.unique())\n",
    "    puzzle_df.response_type = [response_map[response] for response in puzzle_df.response_type]\n",
    "    puzzle_df = puzzle_df.sort_values(['subject_id', 'trial'])\n",
    "    responses = torch.tensor(puzzle_df.response_type.to_numpy(), device=device).view(-1, num_trials)\n",
    "    subject_ids = puzzle_df.subject_id.unique()\n",
    "    return responses, subject_ids\n",
    "\n",
    "def split_train_test(subject_responses):\n",
    "    \"\"\"\n",
    "    Splits the subject responses data into an 80/20 split, where every 5 trials,\n",
    "        one is randomly selected to be part of the test set.\n",
    "    First trial is always part of the training set.\n",
    "    \n",
    "    Returns a TensorDict containing\n",
    "        test_trials: tensor of shape [num_subjects, 5] containing the indices of the 5 test trials\n",
    "        train_mask: tensor of shape [num_subjects, 25] containing booleans for whether or not\n",
    "            the trial is part of the training set\n",
    "        train_responses: tensor of shape [num_subjects, 25] where it's the same as subject_responses,\n",
    "            except that the test trials now have -1\n",
    "        train_oh_responses: a one-hot version of train_responses, except test trials are 0 vectors\n",
    "    \"\"\"\n",
    "    device = subject_responses.device\n",
    "    # Mask the subject responses\n",
    "    test_trials = 5 * tu.batch_arange(num_subjects, 5, device=device) \n",
    "    test_trials[:,0] += torch.randint(1, 5, (num_subjects, ), device=device)\n",
    "    test_trials[:,1:] += torch.randint(0, 5, (num_subjects, 4), device=device)\n",
    "    train_mask = torch.ones_like(subject_responses)\n",
    "    for i in range(test_trials.shape[-1]):\n",
    "        tu.write_subtensors(train_mask, test_trials[:,i],\n",
    "                                     torch.zeros(num_subjects, dtype=int, device=device))\n",
    "\n",
    "    train_responses = (subject_responses * train_mask) - (1 - train_mask)\n",
    "    oh_responses = tu.one_hot_encode(subject_responses)\n",
    "    train_oh_responses = tu.one_hot_encode(subject_responses) * train_mask.unsqueeze(-1)\n",
    "    return TensorDict(test_trials=test_trials,\n",
    "                      train_mask=train_mask,\n",
    "                      train_responses=train_responses,\n",
    "                      train_oh_responses=train_oh_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_strat_responses(p_lapse_ih, p_lapse_a, p_lapse_d):\n",
    "    \"\"\"\n",
    "    Returns P(response | strategy), i.e. probabilities of each response type given strategy\n",
    "    \"\"\"\n",
    "    device = p_lapse_ih.device\n",
    "    p = torch.tensor([[3/9, 4/9, 1/9, 1/9],\n",
    "                      [1,   4/6, 1/6, 1/6],\n",
    "                      [1,   1,   1/2, 1/2],\n",
    "                      [1,   1,   1,   1  ]], device=device)\n",
    "    \n",
    "    # for gradient to flow, need to assign values like this\n",
    "    multipliers = [[1,          1,              1,                          1],\n",
    "                   [p_lapse_ih, 1 - p_lapse_ih, 1 - p_lapse_ih,             1 - p_lapse_ih],\n",
    "                   [p_lapse_ih, p_lapse_a,      1 - p_lapse_ih - p_lapse_a, 1 - p_lapse_ih - p_lapse_a],\n",
    "                   [p_lapse_ih, p_lapse_a,      p_lapse_d,                  1 - p_lapse_ih - p_lapse_a - p_lapse_d]]\n",
    "    multipliers = tu.make_tensor(multipliers, dtype=torch.float32, device=device)\n",
    "    p = p * multipliers\n",
    "    return p\n",
    "    \n",
    "\n",
    "def zero_p_transitions(p_transitions):\n",
    "    assert p_transitions.shape == (4, 4)\n",
    "    M = torch.zeros_like(p_transitions)\n",
    "    for i in range(0, 4):\n",
    "        M[i, i:] = p_transitions[i, i:].softmax(-1)\n",
    "    return M\n",
    "\n",
    "def get_p_strategies(subject_responses,\n",
    "                     p_strat_trial0,\n",
    "                     p_transitions):\n",
    "    \"\"\"\n",
    "    Returns P(strategy | trial)\n",
    "    First, gets P(strategy | trial=1) by solving for\n",
    "        P(response | strategy) * P(strategy) = P(data)\n",
    "        at trial = 1.\n",
    "    Then uses transition matrix to get subsequent state distributions under Markov assumption.\n",
    "    \"\"\"\n",
    "    num_subjects, num_trials = subject_responses.shape\n",
    "    p_s = p_strat_trial0\n",
    "    p_strategies = [p_s]\n",
    "    \n",
    "    # use transition matrix to derive P(strategy | trial)\n",
    "    for i in range(num_trials-1):\n",
    "        p_s = p_transitions.T.matmul(p_s)\n",
    "        p_strategies.append(p_s)\n",
    "    p_strategies = torch.stack(p_strategies, dim=0)\n",
    "    return p_strategies\n",
    "\n",
    "def get_p_responses(p_strat_responses, p_strategies):\n",
    "    \"\"\"\n",
    "    Returns P(response | trial) by computing\n",
    "        sum{strategy} P(response | strategy, trial) * P(strategy | trial)\n",
    "    \"\"\"\n",
    "    return p_strategies.matmul(p_strat_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macromodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MacroModel(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 subject_responses,\n",
    "                 p_lapse_ih=0.1,\n",
    "                 p_lapse_a=0.1,\n",
    "                 p_lapse_d=0.1):\n",
    "        # cap p_lapse at 20%, though it wouldn't actually reach it, just to keep things somewhat reasonable\n",
    "        assert 0 < p_lapse_ih <= .2\n",
    "        assert 0 < p_lapse_a <= .2\n",
    "        assert 0 < p_lapse_d <= .2\n",
    "        \n",
    "        super().__init__()\n",
    "        self.subject_responses = subject_responses\n",
    "        \n",
    "        self.p_lapse_ih = nn.Parameter(tu.logit(torch.tensor(5 * p_lapse_ih))) # max it at 20%\n",
    "        self.p_lapse_a = nn.Parameter(tu.logit(torch.tensor(5 * p_lapse_a))) # max it at 20%\n",
    "        self.p_lapse_d = nn.Parameter(tu.logit(torch.tensor(5 * p_lapse_d))) # max it at 20%\n",
    "        self.p_strat_trial0 = nn.Parameter(torch.rand(4))\n",
    "        self.p_transitions = nn.Parameter(torch.rand(4, 4))\n",
    "        \n",
    "    def get_params(self):\n",
    "        return TensorDict(p_lapse_ih = self.p_lapse_ih.sigmoid() / 5,\n",
    "                          p_lapse_a = self.p_lapse_a.sigmoid() / 5,\n",
    "                          p_lapse_d = self.p_lapse_d.sigmoid() / 5,\n",
    "                          p_strat_trial0 = self.p_strat_trial0.softmax(-1),\n",
    "                          p_transitions = zero_p_transitions(self.p_transitions))\n",
    "        \n",
    "    def forward(self, train_mask=None):\n",
    "        \"\"\"\n",
    "        In the forward calculation, only the first trial is used (to establish prior)\n",
    "        \n",
    "        train_mask: boolean tensor with shape [num_subjects, num_trials] indicating which trials should be\n",
    "            used for training\n",
    "            If None, all trials are used for training\n",
    "        \"\"\"\n",
    "        num_subjects, num_trials = self.subject_responses.shape\n",
    "        params = self.get_params()\n",
    "\n",
    "        p_strat_responses = get_p_strat_responses(params.p_lapse_ih,\n",
    "                                                  params.p_lapse_a,\n",
    "                                                  params.p_lapse_d)\n",
    "        p_strategies = get_p_strategies(self.subject_responses, params.p_strat_trial0, params.p_transitions)\n",
    "        p_responses = get_p_responses(p_strat_responses, p_strategies)\n",
    "        \n",
    "        p_responses_exp = tu.expand_along_dim(p_responses, 0, num_subjects)\n",
    "        nll = tu.nll_loss(p_responses_exp.log(), subject_responses, reduction='none')\n",
    "        \n",
    "        if train_mask is not None:\n",
    "            nll = (nll * train_mask).sum() / train_mask.sum()\n",
    "        else:\n",
    "            nll = nll.mean()\n",
    "        \n",
    "        return TensorDict(p_strat_responses=p_strat_responses,\n",
    "                          p_strategies=p_strategies,\n",
    "                          p_responses=p_responses,\n",
    "                          nll=nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_macro_model(subject_responses,\n",
    "                      num_epochs=1000,\n",
    "                      lr=.01,\n",
    "                      train_all=False,\n",
    "                      show_pbar=False,\n",
    "                      print_epochs=0):\n",
    "    \n",
    "    model = MacroModel(subject_responses)\n",
    "    model = model.to(subject_responses.device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    split_responses = split_train_test(subject_responses)\n",
    "    \n",
    "    iterable = range(num_epochs + 1)\n",
    "    \n",
    "    if show_pbar > 0:\n",
    "        iterable = tqdm(iterable)\n",
    "        \n",
    "    for epoch in iterable:\n",
    "        optimizer.zero_grad()\n",
    "        results = model(None if train_all else split_responses.train_mask)\n",
    "        loss = results.nll\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if print_epochs > 0 and (epoch%print_epochs == 0 or epoch == num_epochs):\n",
    "            print(\"Epoch: {}, NLL: {}\".format(epoch, loss))\n",
    "            print(results.p_strategies[0])\n",
    "            model.get_params().print(round_digits=4)\n",
    "            \n",
    "    return model, split_responses, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Micromodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_paths(subject_responses,\n",
    "                p_strat_priors,\n",
    "                p_strat_responses,\n",
    "                p_transitions,\n",
    "                train_mask=None,\n",
    "                show_pbar=False):\n",
    "    \"\"\"\n",
    "    Returns a TensorDict with\n",
    "        paths: tensor of shape [num_paths, num_trials] indicating the strategy at each step\n",
    "        p_paths: tensor of shape [num_subjects, num_paths] with probability of each path\n",
    "        \n",
    "    train_mask: boolean tensor of shape [num_subjects, num_trials]\n",
    "        Indicates trials to include the likelihood. When false, only the prior is used.\n",
    "        If train_mask is None, likelihood is used in all trials.\n",
    "    \"\"\"\n",
    "    # all operations are done sequentially, so moving to cpu is faster\n",
    "    device = subject_responses.device\n",
    "    subject_responses = subject_responses.cpu()\n",
    "    p_transitions = p_transitions.cpu()\n",
    "    p_strat_responses = p_strat_responses.cpu()\n",
    "    \n",
    "    if train_mask is not None:\n",
    "        train_mask = train_mask.cpu()\n",
    "    \n",
    "    # tensor of P(actual response | strategy) for each strategy\n",
    "    # shape [num_subjects, num_trials, num_strategies]  i.e. [88, 25, 4]\n",
    "    likelihoods = tu.prepend_shape(p_strat_responses, subject_responses.shape)\n",
    "    likelihoods = tu.swap_dims(likelihoods, -1, -2)\n",
    "    likelihoods = tu.select_subtensors(likelihoods, subject_responses)\n",
    "\n",
    "    all_p_paths = []\n",
    "\n",
    "    # for each subject, build P(path) using Bayes Theorem\n",
    "    iterable = range(num_subjects)\n",
    "    if show_pbar:\n",
    "        iterable = tqdm(iterable)\n",
    "    for sid in iterable:\n",
    "        p_s = likelihoods[sid, 0] * p_strat_priors\n",
    "        p_s = p_s / p_s.sum(-1).unsqueeze(-1)\n",
    "        p_paths = p_s\n",
    "        paths = [(0,), (1,), (2,), (3,)]\n",
    "\n",
    "        for trial in range(1, num_trials):\n",
    "            new_paths = []\n",
    "            new_p_paths = []\n",
    "\n",
    "            for i, strat in itertools.product(range(len(paths)), range(4)):\n",
    "                path = paths[i]\n",
    "                \n",
    "                # reject paths that switch to a worse strategy\n",
    "                # they need to be left out of the loop so that they aren't included in the normalizing sum (denominator)\n",
    "                if path[-1] > strat:\n",
    "                    continue\n",
    "\n",
    "                # P(path_{t-1}) x P(s | s_{t-1}) x P(r | s)\n",
    "                # Only apply likelihood if the trial is part of the training set\n",
    "                p_path = p_paths[i] * p_transitions[path[-1], strat]\n",
    "                if train_mask is None or train_mask[sid, trial]:\n",
    "                    p_path = p_path * likelihoods[sid, trial, strat]\n",
    "                new_p_paths.append(p_path)\n",
    "\n",
    "                new_paths.append(path + (strat, ))\n",
    "\n",
    "            paths = new_paths\n",
    "            p_paths = torch.stack(new_p_paths, dim=0)\n",
    "            p_paths = p_paths / p_paths.sum()\n",
    "\n",
    "        all_p_paths.append(p_paths)\n",
    "    \n",
    "    all_p_paths = torch.stack(all_p_paths, dim=0)\n",
    "    paths = torch.tensor(paths)\n",
    "    \n",
    "    oh_paths = tu.one_hot_encode(paths)\n",
    "    p_strategies = tu.extend_mul(all_p_paths, oh_paths).sum(1)\n",
    "    p_responses = tu.extend_mul(p_strategies, p_strat_responses).sum(2)\n",
    "    return TensorDict(paths=paths,\n",
    "                      probs=all_p_paths,\n",
    "                      p_strategies=p_strategies,\n",
    "                      p_responses=p_responses).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_responses(micro_models, num_samples):\n",
    "    \"\"\"\n",
    "    For each participant, takes their fitted HMM and samples a strategy path.\n",
    "    Then using those strategy paths, samples responses for each trial.\n",
    "    \"\"\"\n",
    "    paths = tu.prepend_shape(micro_models.paths, (num_samples, num_subjects))\n",
    "    path_distributions = Categorical(probs = micro_models.probs)\n",
    "    sample_paths = path_distributions.sample((num_samples,))\n",
    "    sample_strategies = tu.select_subtensors(paths, sample_paths)\n",
    "    p_strat_responses_exp = tu.prepend_shape(p_strat_responses, sample_strategies.shape)\n",
    "    p_responses = tu.select_subtensors(p_strat_responses_exp, sample_strategies)\n",
    "    response_distributions = Categorical(probs = p_responses)\n",
    "    sample_responses = response_distributions.sample()\n",
    "    sample_responses = tu.swap_dims(sample_responses, 0, 1)\n",
    "    return sample_responses\n",
    "\n",
    "def compare_actual_nll(micro_models, subject_responses, num_samples, train_mask=None):\n",
    "    \"\"\"\n",
    "    do monte carlo to get NLL distribution of model samples for each subject\n",
    "    Also returns percentiles of actual data's NLL. Returns a range for valid percentiles:\n",
    "        percentile_min: % of samples with NLL < actual data's\n",
    "        percentile_max: 1 - % of samples with NLL > actual data's\n",
    "        percentile_sample: a random number between percentile_min and percentile_max\n",
    "    \"\"\" \n",
    "    samples = sample_responses(micro_models, num_samples)\n",
    "\n",
    "    p_responses = tu.expand_along_dim(micro_models.p_responses, 1, num_samples)\n",
    "    sample_nll = tu.nll_loss(p_responses.log(), samples, reduction='none')\n",
    "    actual_nll = tu.nll_loss(micro_models.p_responses.log(), subject_responses, reduction='none')\n",
    "    \n",
    "    td = TensorDict()\n",
    "    \n",
    "    if train_mask is not None:\n",
    "        test_mask = 1 - train_mask\n",
    "        sample_test_nll = (sample_nll * test_mask.unsqueeze(1)).sum(-1)\n",
    "        actual_test_nll = (actual_nll * test_mask).sum(-1)\n",
    "        percentile_min = (sample_test_nll < actual_test_nll.unsqueeze(-1)).float().mean(-1)\n",
    "        percentile_max = 1 - (sample_test_nll > actual_test_nll.unsqueeze(-1)).float().mean(-1)\n",
    "        percentile_sample = Uniform(percentile_min, percentile_max).sample()\n",
    "        \n",
    "        td.test = TensorDict(sample_nll=sample_test_nll,\n",
    "                             actual_nll=actual_test_nll,\n",
    "                             percentile_min=percentile_min,\n",
    "                             percentile_max=percentile_max,\n",
    "                             percentile_sample=percentile_sample)\n",
    "    \n",
    "    sample_nll = sample_nll.sum(-1)\n",
    "    actual_nll = actual_nll.sum(-1)\n",
    "    percentile_min = (sample_nll < actual_nll.unsqueeze(-1)).float().mean(-1)\n",
    "    percentile_max = 1 - (sample_nll > actual_nll.unsqueeze(-1)).float().mean(-1)\n",
    "    percentile_sample = Uniform(percentile_min, percentile_max).sample()\n",
    "    \n",
    "    td.all = TensorDict(sample_nll=sample_nll,\n",
    "                        actual_nll=actual_nll,\n",
    "                        percentile_min=percentile_min,\n",
    "                        percentile_max=percentile_max,\n",
    "                        percentile_sample=percentile_sample)\n",
    "    return td"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_metrics(p_responses, subject_responses, train_mask, num_samples):\n",
    "    \"\"\"\n",
    "    Returns NLL of actual samples and percentiles of actual sample NLL\n",
    "    \n",
    "    p_responses: tensor of shape [num_subjects, num_trials, 4]\n",
    "    train_mask: boolean tensor of shape [num_subjects, num_trials]\n",
    "    num_samples: int\n",
    "    \"\"\"\n",
    "    num_subjects, num_trials = subject_responses.shape\n",
    "    assert p_responses.shape == (num_subjects, num_trials, 4)\n",
    "    assert train_mask.shape == (num_subjects, num_trials)\n",
    "    \n",
    "    # get actual data NLL\n",
    "    actual_nll = tu.nll_loss(p_responses.log(), subject_responses, reduction='none')\n",
    "    actual_nll = (actual_nll * (1 - train_mask)).sum(-1)\n",
    "    \n",
    "    # get sample data NLL\n",
    "    samples = Categorical(p_responses).sample((num_samples, ))\n",
    "    p_responses = tu.prepend_shape(p_responses, num_samples)\n",
    "    sample_nll = tu.nll_loss(p_responses.log(), samples, reduction='none')\n",
    "    train_mask = tu.prepend_shape(train_mask, num_samples)\n",
    "    sample_nll = (sample_nll * (1 - train_mask)).sum(-1)\n",
    "    \n",
    "    # calculate percentile range and sample a value\n",
    "    percentile_min = (sample_nll < actual_nll.unsqueeze(0)).float().mean(0)\n",
    "    percentile_max = 1 - (sample_nll > actual_nll.unsqueeze(0)).float().mean(0)\n",
    "    percentile = Uniform(percentile_min, percentile_max).sample()\n",
    "    \n",
    "    return TensorDict(actual_nll=actual_nll, sample_nll=sample_nll.T, percentile=percentile)\n",
    "\n",
    "def make_df(metrics, model_name):\n",
    "    actual_df = metrics[['actual_nll', 'percentile']].to_dataframe({0: 'sim', 1: 'subject_id'})\n",
    "    sample_df = metrics[['sample_nll']].to_dataframe({0: 'sim', 1: 'subject_id', 2: 'sample_id'})\n",
    "    actual_df = actual_df.rename({'actual_nll': 'nll'}, axis=1)\n",
    "    sample_df = sample_df.rename({'sample_nll': 'nll'}, axis=1)\n",
    "    actual_df['model'] = model_name\n",
    "    sample_df['model'] = model_name\n",
    "    return actual_df, sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "solvers = True\n",
    "num_runs = 50 # For cross-validation, how many separate models to train\n",
    "num_samples = 100 # For cross-validation, how many samples from each model to draw\n",
    "\n",
    "# Path of data files\n",
    "puzzle_path = \"data/processed/puzzle_data.tsv\"\n",
    "subject_path = \"data/processed/subject_data.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "subject_responses, subject_ids = load_responses(puzzle_path, subject_path, solvers=solvers, device=device)\n",
    "num_subjects, num_trials = subject_responses.shape\n",
    "\n",
    "# Create save directory\n",
    "group = 'solvers' if solvers else 'nonsolvers'\n",
    "dirpath = proj_path + 'data/hmm/{}/'.format(group)\n",
    "utils.mkdir(dirpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models without held-out data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93dd7e587cf4f1c957ad357f06d657f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2001.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d5ef2877ac4529a8d302ce896e6025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=88.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train models\n",
    "\n",
    "macro_model, splits, results = train_macro_model(subject_responses,\n",
    "                                                 train_all=True,\n",
    "                                                 num_epochs=2000,\n",
    "                                                 show_pbar=True)\n",
    "params = macro_model.get_params().detach()\n",
    "p_strat_responses = results.p_strat_responses.detach()\n",
    "micro_model = get_p_paths(subject_responses,\n",
    "                          params.p_strat_trial0.cpu(),\n",
    "                          p_strat_responses,\n",
    "                          params.p_transitions,\n",
    "                          train_mask=None,\n",
    "                          show_pbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P(response) and P(strategy) for each participant\n",
    "\n",
    "p_responses = utils.to_dataframe(micro_model.p_responses, ['subject_id', 'trial', 'response_type'], 'probability')\n",
    "p_responses.subject_id = [subject_ids[s] for s in p_responses.subject_id]\n",
    "p_responses.to_csv(dirpath + \"subject_p_responses.tsv\".format(group), sep='\\t', index=False)\n",
    "\n",
    "p_strategies = utils.to_dataframe(micro_model.p_strategies, ['subject_id', 'trial', 'strategy'], 'probability')\n",
    "p_strategies.subject_id = [subject_ids[s] for s in p_strategies.subject_id]\n",
    "p_strategies.to_csv(dirpath + \"subject_p_strategies.tsv\".format(group), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top posterior strategy paths for each participant\n",
    "\n",
    "p_paths, path_idx = micro_model.probs.sort(-1, descending=True)\n",
    "p_paths = p_paths[:,:5]\n",
    "path_idx = path_idx[:,:5]\n",
    "\n",
    "p_paths = utils.to_dataframe(p_paths, ['subject_id', 'rank'], 'probability')\n",
    "p_paths.subject_id = [subject_ids[s] for s in p_paths.subject_id]\n",
    "p_paths.to_csv(dirpath + \"top_paths_probs.tsv\", sep='\\t', index=False)\n",
    "\n",
    "top_paths = []\n",
    "for i in range(num_subjects):\n",
    "    idx = path_idx[i]\n",
    "    top_paths.append(micro_model.paths[idx])\n",
    "top_paths = torch.stack(top_paths, dim=0)\n",
    "top_paths = utils.to_dataframe(top_paths, ['subject_id', 'rank', 'trial'], 'strategy')\n",
    "top_paths.subject_id = [subject_ids[s] for s in top_paths.subject_id]\n",
    "top_paths.to_csv(dirpath + \"top_paths.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models with held-out data for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e8bc7f4eee4582abc01a5e49af9c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train models\n",
    "\n",
    "split_responses = []\n",
    "macro_models = []\n",
    "micro_models = []\n",
    "\n",
    "for i in tqdm(range(num_runs)):\n",
    "    macro_model, splits, results = train_macro_model(subject_responses, num_epochs=2000)\n",
    "    params = macro_model.get_params().detach()\n",
    "    p_strat_responses = results.p_strat_responses.detach()\n",
    "    micro_model = get_p_paths(subject_responses,\n",
    "                              params.p_strat_trial0.cpu(),\n",
    "                              p_strat_responses,\n",
    "                              params.p_transitions,\n",
    "                              train_mask=splits.train_mask)\n",
    "    \n",
    "    split_responses.append(splits)\n",
    "    macro_models.append(macro_model)\n",
    "    micro_models.append(micro_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLL for micro models\n",
    "# Does hierarchical sampling so has a slight different procedure than other models\n",
    "\n",
    "micro_metrics = []\n",
    "for i in range(num_runs):\n",
    "    comparisons = compare_actual_nll(micro_models[i],\n",
    "                                     subject_responses,\n",
    "                                     num_samples=num_samples,\n",
    "                                     train_mask=split_responses[i].train_mask)\n",
    "    td = TensorDict(actual_nll=comparisons.test.actual_nll,\n",
    "                    sample_nll=comparisons.test.sample_nll,\n",
    "                    percentile=comparisons.test.percentile_sample)\n",
    "    micro_metrics.append(td)\n",
    "micro_metrics = TensorDict.stack(micro_metrics, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLL for priors model\n",
    "\n",
    "macro_metrics = []\n",
    "for i in range(num_runs):\n",
    "    outputs = macro_models[i]()\n",
    "    p_responses = tu.prepend_shape(outputs.p_responses, num_subjects)\n",
    "    metrics = get_cv_metrics(p_responses, subject_responses, split_responses[i].train_mask, num_samples)\n",
    "    macro_metrics.append(metrics)\n",
    "macro_metrics = TensorDict.stack(macro_metrics, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLL for means model\n",
    "means_metrics = []\n",
    "eps = 1e-3 # some train/test splits can remove all instances of a response type in a trial, making p = 0\n",
    "for i in range(num_runs):\n",
    "    train_responses = eps + split_responses[i].train_oh_responses.sum(0).float()\n",
    "    avg_responses = train_responses / train_responses.sum(-1).unsqueeze(-1)\n",
    "    p_responses = tu.prepend_shape(avg_responses, num_subjects)\n",
    "    metrics = get_cv_metrics(p_responses, subject_responses, split_responses[i].train_mask, num_samples)\n",
    "    means_metrics.append(metrics)\n",
    "means_metrics = TensorDict.stack(means_metrics, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "\n",
    "dfs = [make_df(*args) for args in [(micro_metrics, 'hmm'),\n",
    "                                   (macro_metrics, 'priors'),\n",
    "                                   (means_metrics, 'means')]]\n",
    "actual_df = pd.concat([df[0] for df in dfs])\n",
    "sample_df = pd.concat([df[1] for df in dfs])\n",
    "actual_df.subject_id = [subject_ids[i] for i in actual_df.subject_id]\n",
    "sample_df.subject_id = [subject_ids[i] for i in sample_df.subject_id]\n",
    "\n",
    "\n",
    "actual_df.to_csv(dirpath + 'nll_actual.tsv', sep='\\t', index=False)\n",
    "sample_df.to_csv(dirpath + 'nll_sample.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
