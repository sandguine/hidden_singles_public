{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_path = '/home/ajhnam/projects/hidden_singles_public/'\n",
    "save_path = '/data2/pdp/ajhnam/hidden_singles_public/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(proj_path + 'python/')\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import copy\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader as DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "from hiddensingles.misc import torch_utils as tu\n",
    "from hiddensingles.misc import pd_utils as pu\n",
    "from hiddensingles.misc import utils, TensorDict, TensorDictDataset, RRN, DigitRRN, GPUMultiprocessor\n",
    "from hiddensingles.experiment.sudoku_hs_service import create_tutorial, create_phase1, create_phase2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(model, dataset, num_steps=8):\n",
    "    outputs = model(dataset.inputs, num_steps=num_steps)\n",
    "    \n",
    "    goals = tu.expand_along_dim(dataset.goals, 1, num_steps)\n",
    "    goal_outputs = tu.select(outputs, goals, select_dims=1)\n",
    "    \n",
    "    targets = tu.expand_along_dim(dataset.targets, 1, num_steps)\n",
    "    goal_loss = tu.cross_entropy(goal_outputs, targets)\n",
    "    goal_probs = tu.select(goal_outputs.softmax(-1), dataset.targets)\n",
    "    goal_td = TensorDict(loss=goal_loss,\n",
    "                         probs=goal_probs,\n",
    "                         outputs=goal_outputs)\n",
    "    \n",
    "    coords = tu.expand_along_dim(dataset.coords, 1, num_steps)\n",
    "    out_exp = tu.expand_along_dim(outputs, 2, 9)\n",
    "    coord_outputs = tu.select(out_exp, coords, select_dims=1)\n",
    "    coord_targets = tu.expand_along_dim(dataset.coord_targets, 1, num_steps)\n",
    "    coord_loss = tu.cross_entropy(coord_outputs, coord_targets)\n",
    "    coord_probs = tu.select_subtensors(coord_outputs.softmax(-1), coord_targets)\n",
    "    coord_td = TensorDict(loss=coord_loss,\n",
    "                          probs=coord_probs,\n",
    "                          outputs=coord_outputs)\n",
    "\n",
    "    loss = goal_loss + coord_loss\n",
    "    correct = dataset.targets == goal_outputs[:,-1].argmax(-1)\n",
    "    return TensorDict(loss=loss,\n",
    "                      correct=correct,\n",
    "                      outputs=outputs,\n",
    "                      goal=goal_td,\n",
    "                      coord=coord_td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_performance(model, dataset, df_conditions, model_type, run_id, num_train, epoch):\n",
    "    # load dataset\n",
    "    train_dset = dataset.train[:num_train]\n",
    "    valid_dset = dataset.valid\n",
    "    test_dset = dataset.test\n",
    "    \n",
    "    # get results\n",
    "    with torch.no_grad():\n",
    "        train_results = get_results(model, train_dset)\n",
    "        valid_results = get_results(model, valid_dset)\n",
    "        test_results = get_results(model, test_dset)\n",
    "\n",
    "    df_train = train_results[['correct']].to_dataframe({0: 'puzzle_id'})\n",
    "    df_valid = valid_results[['correct']].to_dataframe({0: 'puzzle_id'})\n",
    "    df_test = test_results[['correct']].to_dataframe({0: 'puzzle_id'})\n",
    "    df_train['dataset'] = 'train'\n",
    "    df_valid['dataset'] = 'valid'\n",
    "    df_train = pd.concat([df_train, df_valid])\n",
    "    \n",
    "    df_train['run_id'] = run_id\n",
    "    df_train['model'] = model_type\n",
    "    df_train['num_train'] = num_train\n",
    "    df_train['epoch'] = epoch\n",
    "    df_test['run_id'] = run_id\n",
    "    df_test['model'] = model_type\n",
    "    df_test['num_train'] = num_train\n",
    "    df_test['epoch'] = epoch\n",
    "    \n",
    "    df_train = df_train[['run_id', 'model', 'num_train', 'epoch', 'dataset', 'puzzle_id', 'correct']]\n",
    "    df_test = df_test[['run_id', 'model', 'num_train', 'epoch', 'puzzle_id', 'correct']]\n",
    "    df_test = df_test.merge(df_conditions, on=['run_id', 'puzzle_id'])\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phase2_conditions(phase2):\n",
    "    ht = [p.condition.house_type for p in phase2]\n",
    "    hi = [p.condition.house_index for p in phase2]\n",
    "    ci = [p.condition.cell_index for p in phase2]\n",
    "    ds = [p.condition.digit_set for p in phase2]\n",
    "    conditions = pd.DataFrame(np.array([ht, hi, ci, ds]).T,\n",
    "                              columns=['house_type', 'house_index', 'cell_index', 'digit_set'])\n",
    "    return conditions\n",
    "\n",
    "def hidden_singles_to_tensordict(list_of_hidden_singles, digit_rrn, device='cpu'):\n",
    "    grids = torch.tensor([a.grid.array for a in list_of_hidden_singles], device=device)\n",
    "    goals = [p.coordinates['goal'] for p in list_of_hidden_singles]\n",
    "    goals = torch.tensor([[g.x, g.y] for g in goals], device=device)\n",
    "    targets = torch.tensor([a.digits['target'] for a in list_of_hidden_singles], device=device) - 1 # make it 0-8\n",
    "    coords = grids.nonzero()[:,1:].view(len(list_of_hidden_singles), -1, 2)\n",
    "    coord_targets = tu.select(tu.expand_along_dim(grids, 1, 9), coords) - 1 # make it 0-8\n",
    "    \n",
    "    inputs = DigitRRN.make_onehot(grids) if digit_rrn else grids\n",
    "    return TensorDict(inputs=inputs,\n",
    "                      grids=grids,\n",
    "                      goals=goals,\n",
    "                      targets=targets,\n",
    "                      coords=coords,\n",
    "                      coord_targets=coord_targets)\n",
    "\n",
    "def create_dataset(num_phase1):\n",
    "    digit_set1 = set(random.sample(set(range(1, 10)), 4))\n",
    "    digit_set2 = set(random.sample(set(range(1, 10)) - digit_set1, 4))\n",
    "    tutorial = create_tutorial(digit_set1)\n",
    "    phase1 = create_phase1(tutorial, num_phase1)\n",
    "    phase2 = create_phase2(tutorial, digit_set1, digit_set2)\n",
    "    conditions = get_phase2_conditions(phase2)\n",
    "    return phase1, phase2, conditions\n",
    "\n",
    "def train_loop(model, dataloader, optimizer, num_steps=8):\n",
    "    for dset in dataloader:\n",
    "        dset = TensorDict(**dset)\n",
    "        optimizer.zero_grad()\n",
    "        results = get_results(model, dset, num_steps=num_steps)\n",
    "        results.loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_id, num_train, model_type, device):\n",
    "    assert model_type in ('rrn', 'drrn')\n",
    "    time_start = datetime.now()\n",
    "    print(\"{} Starting run: {}, model: {}, num_train: {}, device: {}\".format(\n",
    "        time_start.strftime(\"%Y-%m-%d %H:%M:%S\"), run_id, model_type, num_train, device), end='\\n')\n",
    "    dirpath = save_path + '{}_hs/tr{}_rid{}/'.format(model_type, num_train, run_id)\n",
    "    utils.mkdir(dirpath)\n",
    "    \n",
    "    if model_type == 'rrn':\n",
    "        model = RRN(digit_embed_size=10,\n",
    "                    num_mlp_layers=0,\n",
    "                    hidden_vector_size=48,\n",
    "                    message_size=48,\n",
    "                    encode_coordinates=False).to(device)\n",
    "        batch_size = 100\n",
    "    else:\n",
    "        model = DigitRRN(hidden_vector_size=16,\n",
    "                         message_size=16).to(device)\n",
    "        batch_size = 10\n",
    "    \n",
    "    num_epochs = 1000\n",
    "    dataset = TensorDict.load(save_path + 'hs_data.td')\n",
    "    dataset = dataset[model_type][run_id].to(device)\n",
    "    dataloader = DataLoader(TensorDictDataset(dataset.train[:num_train]),\n",
    "                            batch_size=batch_size, shuffle=True)\n",
    "    df_conditions = pd.read_csv(save_path + 'hs_conditions.tsv', sep='\\t')\n",
    "    df_conditions = df_conditions[df_conditions.run_id == run_id]\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    \n",
    "    df_train, df_test = get_model_performance(model, dataset, df_conditions,\n",
    "                                              model_type, run_id, num_train, 0)\n",
    "    df_train.to_csv(dirpath + 'tr_results_ep_0.tsv', sep='\\t', index=False)\n",
    "    df_test.to_csv(dirpath + 'te_results_ep_0.tsv', sep='\\t', index=False)\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loop(model, dataloader, optimizer)\n",
    "        if epoch%10 == 0:\n",
    "            df_train, df_test = get_model_performance(model, dataset, df_conditions, \n",
    "                                                      model_type, run_id, num_train, epoch)\n",
    "            df_train.to_csv(dirpath + 'tr_results_ep_{}.tsv'.format(epoch), sep='\\t', index=False)\n",
    "            df_test.to_csv(dirpath + 'te_results_ep_{}.tsv'.format(epoch), sep='\\t', index=False)\n",
    "            torch.save(model.state_dict(),\n",
    "                       os.path.join(dirpath, 'epoch_{}.mdl'.format(epoch)))\n",
    "            \n",
    "    time_end = datetime.now()\n",
    "    elapsed = str(time_end - time_start)\n",
    "    print(\"{} Completed in {}. run: {}, model: {}, num_train: {}, device: {}\".format(\n",
    "        time_end.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        str(elapsed),\n",
    "        run_id, model_type, num_train, device), end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models\n",
    "\n",
    "Creating dataset and training models should happen only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 10\n",
    "num_train = 500\n",
    "num_valid = 100\n",
    "load = True\n",
    "\n",
    "if load:\n",
    "    dataset = TensorDict.load(save_path + 'hs_data.td')\n",
    "    df_conditions = pd.read_csv(save_path + 'hs_conditions.tsv', sep='\\t')\n",
    "else:\n",
    "    datasets = []\n",
    "    all_conditions = []\n",
    "    for i in range(num_runs):\n",
    "        phase1, phase2, conditions = create_dataset(num_phase1=num_train+num_valid)\n",
    "        conditions['run_id'] = i\n",
    "        conditions['puzzle_id'] = range(64)\n",
    "\n",
    "        rrn_phase1 = hidden_singles_to_tensordict(phase1, digit_rrn=False)\n",
    "        rrn_phase2 = hidden_singles_to_tensordict([p.hidden_single for p in phase2], digit_rrn=False)\n",
    "        drrn_phase1 = hidden_singles_to_tensordict(phase1, digit_rrn=True)\n",
    "        drrn_phase2 = hidden_singles_to_tensordict([p.hidden_single for p in phase2], digit_rrn=True)\n",
    "        rrn_dset = TensorDict(train=rrn_phase1[:num_train],\n",
    "                              valid=rrn_phase1[num_train:],\n",
    "                              test=rrn_phase2)\n",
    "        drrn_dset = TensorDict(train=drrn_phase1[:num_train],\n",
    "                               valid=drrn_phase1[num_train:],\n",
    "                               test=drrn_phase2)\n",
    "        dset = TensorDict(rrn=rrn_dset, drrn=drrn_dset)\n",
    "        datasets.append(dset)\n",
    "        all_conditions.append(conditions)\n",
    "        \n",
    "    dataset = TensorDict.stack(datasets, 0)\n",
    "    dataset.save(save_path + 'hs_data.td')\n",
    "    df_conditions = pd.concat(all_conditions)\n",
    "    df_conditions.to_csv(save_path + 'hs_conditions.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_kwargs = pu.crossing(run_id=range(10),\n",
    "                        num_train=[25, 50, 100, 300, 500],\n",
    "                        model_type=['rrn', 'drrn'])\n",
    "mp = GPUMultiprocessor(run, df_kwargs, devices=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "mp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_results = []\n",
    "te_results = []\n",
    "for filename in tqdm(glob.glob('/data2/pdp/ajhnam/hidden_singles_public/*_hs/**/tr_results_*.tsv')):\n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "    tr_results.append(df)\n",
    "for filename in tqdm(glob.glob('/data2/pdp/ajhnam/hidden_singles_public/*_hs/**/te_results_*.tsv')):\n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "    te_results.append(df)\n",
    "tr_results = pd.concat(tr_results)\n",
    "te_results = pd.concat(te_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_results.to_csv('/data2/pdp/ajhnam/hidden_singles_public/rrn_hs_tr_results.tsv', sep='\\t', index=False)\n",
    "te_results.to_csv('/data2/pdp/ajhnam/hidden_singles_public/rrn_hs_te_results.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
