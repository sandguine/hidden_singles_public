{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_path = '/home/ajhnam/projects/hidden_singles_public/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(proj_path + 'python/')\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader as DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from hiddensingles.misc import torch_utils as tu\n",
    "from hiddensingles.misc import utils, TensorDict, TensorDictDataset, RRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(model, dataset, batch_size, num_steps=16, optimizer=None):\n",
    "    train = optimizer is not None\n",
    "    \n",
    "    dataloader = DataLoader(TensorDictDataset(dataset), batch_size=batch_size, shuffle=train)\n",
    "    \n",
    "    losses = []\n",
    "    correct = []\n",
    "    for dset in dataloader:\n",
    "        dset = TensorDict(**dset)\n",
    "        \n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(dset.inputs, num_steps=num_steps)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(dset.inputs, num_steps=num_steps)\n",
    "        outputs = outputs.view(-1, num_steps, model.max_digit, model.max_digit, model.max_digit)\n",
    "        targets = tu.expand_along_dim(dset.targets, 1, num_steps)\n",
    "        loss = tu.cross_entropy(outputs, targets)\n",
    "        \n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # record\n",
    "        losses.append(loss.item())\n",
    "        correct.append((outputs.argmax(-1) == targets)[:,-1])\n",
    "    \n",
    "    correct = torch.cat(correct)\n",
    "    loss = torch.tensor(losses).mean()\n",
    "    accuracy = correct.float().mean().cpu()\n",
    "    solved = correct.all(-1).all(-1).float().mean().cpu()\n",
    "    \n",
    "    results = TensorDict(loss=loss,\n",
    "                         accuracy=accuracy,\n",
    "                         solved=solved)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "train_df = pd.read_csv(proj_path + 'data/rrn/train.csv', names=['input', 'target'])\n",
    "valid_df = pd.read_csv(proj_path + 'data/rrn/valid.csv', names=['input', 'target'])\n",
    "test_df = pd.read_csv(proj_path + 'data/rrn/test.csv', names=['input', 'target'])\n",
    "\n",
    "inputs = torch.tensor(np.array([list(s) for s in train_df.input], dtype=int), device=device).view(-1, 9, 9)\n",
    "targets = torch.tensor(np.array([list(s) for s in train_df.target], dtype=int), device=device).view(-1, 9, 9) - 1\n",
    "train_dset = TensorDict(inputs=inputs, targets=targets)\n",
    "inputs = torch.tensor(np.array([list(s) for s in valid_df.input], dtype=int), device=device).view(-1, 9, 9)\n",
    "targets = torch.tensor(np.array([list(s) for s in valid_df.target], dtype=int), device=device).view(-1, 9, 9) - 1\n",
    "valid_dset = TensorDict(inputs=inputs, targets=targets)\n",
    "inputs = torch.tensor(np.array([list(s) for s in test_df.input], dtype=int), device=device).view(-1, 9, 9)\n",
    "targets = torch.tensor(np.array([list(s) for s in test_df.target], dtype=int), device=device).view(-1, 9, 9) - 1\n",
    "test_dset = TensorDict(inputs=inputs, targets=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RRN(digit_embed_size=10,\n",
    "            num_mlp_layers=0,\n",
    "            hidden_vector_size=96,\n",
    "            message_size=96,\n",
    "            encode_coordinates=False).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-4, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc32798b17b4379a2aa59a9e5f1e610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_steps = 16\n",
    "batch_size = 50\n",
    "num_epochs = 0\n",
    "print_epochs = 1\n",
    "\n",
    "tr_result = get_results(model, train_dset, batch_size=batch_size, num_steps=num_steps)\n",
    "te_result = get_results(model, test_dset, batch_size=batch_size, num_steps=num_steps)\n",
    "tr_results = [tr_result]\n",
    "te_results = [te_result]\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    tr_result = get_results(model, train_dset, batch_size=batch_size, num_steps=num_steps, optimizer=optimizer)\n",
    "    tr_results.append(tr_result)\n",
    "    te_result = get_results(model, test_dset, batch_size=batch_size, num_steps=num_steps)\n",
    "    te_results.append(te_result)\n",
    "\n",
    "    if epoch % print_epochs == 0:\n",
    "        utils.kv_print(epoch=epoch, loss=tr_result.loss,\n",
    "                       tr_acc=tr_result.accuracy, tr_sol=tr_result.solved,\n",
    "                       te_acc=te_result.accuracy, te_sol=te_result.solved)\n",
    "        \n",
    "tr_results = TensorDict.stack(tr_results, 0)\n",
    "te_results = TensorDict.stack(te_results, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df = tr_results.to_dataframe({0: 'epoch'})\n",
    "tr_df['dataset'] = 'train'\n",
    "te_df = te_results.to_dataframe({0: 'epoch'})\n",
    "te_df['dataset'] = 'test'\n",
    "df = pd.concat([tr_df, te_df])\n",
    "\n",
    "df.to_csv(proj_path + \"data/rrn/sudoku_3x3_results.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
